# Experimental Framework for Real-Time Indian Sign Language Recognition Using Advanced Machine Learning Algorithms for Community Development  

## Overview  
This project aims to develop a **real-time sign language interpreter** using:  
- **Flex and accelerometer sensors** integrated with an **ESP32 board**.  
- **Machine learning algorithms** to classify gestures and translate them into Readable outputs.
- Setting up a local server and rendering a webpage to enable real time communication.   
- A solution enabling seamless communication for hearing and speech-impaired individuals.  

## Features  
- **Real-time gesture recognition** using flex and accelerometer sensors.  
- **Compact and portable** design.  
- Operates **offline**, ensuring accessibility anywhere.  
- **Cost-effective** solution for sign language interpretation.  
- **Customizable** for different sign languages.  
- **Accurate recognition** in diverse environmental conditions.  

## System Components  

### Hardware  
- **ESP32 microcontroller**  
- **Flex sensors**  
- **Accelerometer sensor** (e.g., MPU6050)  
- **Power supply**  
- **Hand glove** for mounting sensors
- **Resistors**

  

### Software  
- **Google Collab** for data analysis and machine learning using python.
- Libraries:  
  - `numpy`  
  - `pandas`  
  - `matplotlib`  
  - `scikit-learn`
  - `json`
  - `time`
- Gesture classification using machine learning algorithms such as:  
  - Decision Tree  
  - Random Forest
  - Logistic Regression
  - SVM
  - KNN
  - FNN
  - Transformer model
  - LSTM
  - XGBoost
- **Arduino IDE** for ESP32 programming
- **Tera term** for Data Logging

## Acknowledgments  
Special thanks to:  
- The open-source community for providing tools and libraries that made this project possible.  
- Volunteers who contributed to the data collection.  
